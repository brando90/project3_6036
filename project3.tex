\input 6036tex

\usepackage[pdftex]{graphicx}
\usepackage{amsmath, amsthm, amssymb, amsfonts, mathtools, graphicx, enumerate}
\usepackage{times}
\usepackage{booktabs}
\usepackage{url}
\usepackage{enumerate}
\usepackage{enumitem}

\usepackage{xcolor}

\setlength{\parindent}{0pt}
\setlength{\parskip}{1ex}

\newcommand{\answer}[1]{{\mbox{}\color{red}{#1}}}
\newcommand{\emptycheck}{\text{(\hspace{-.75ex}(\hspace{3ex})\hspace{-.75ex})}}
\newcommand{\checkans}[1]{\text{(\hspace{-.75ex}(\hspace{1ex}{#1}\hspace{1ex})\hspace{-.75ex})}}
\newcommand{\argmax}{{\mbox{arg}\hspace{-.1ex}}\max}

\newcommand{\note}[1]{\textcolor{red}{#1}}
\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\psetheader{Spring 2015}{Project 3: Which movies do Gaussians like?
$ \;$ Issued: Fri.\ 4/10 Due: Fri.\ 4/24 at 9am}

{\bf Project Submission: Please submit two files---a \emph{single} PDF file containing all your answers, code, and graphs, and a
\emph{second} .zip file containing all the code you wrote for this
project, to
the Stellar web site by 9am, April 24th.}

%%
%%%%%%%%%
%%
\textbf{Introduction}

Your task is to build a mixture model for collaborative filtering. You are given a data matrix containing movie ratings made by users; we have sampled this matrix from the Netflix database. Not all movies have been rated by all users, and the goal of this project is to use mixture modeling to predict the missing ratings. You will explore this task by using the Expectation Maximization (EM) algorithm that uses the hidden structure across different users. With the help of this hidden structure you will be able to predict the missing ratings. In other words, we have a partially observed rating matrix and we will use the EM algorithm to fill in the missing entries.
\note{TODO He: remember to put the implementation of K-means from project2 for them please!}

\begin{enumerate}

\item {\bf Part 1} Warm up.

For this part of the project you will compare clustering obtained via K-means to the (soft) clustering induced by EM.

\begin{enumerate}
  \item Use the toy data set (\texttt{toy\_data.txt}) and the K-means code (function \texttt{kMeans} in\\ \texttt{project3\_student.py}) provided to plot different clusters. Notice that when using K-means, each data point is fully assigned to a single cluster, that is, each point can have only one cluster label.\\
  \textbf{Task:} \textbf{Run} the K-means code we provided on the toy data and provide a plot of the clustering. We have provided a function \texttt{init} to initialize \note{TODO He provide initialization to students} K-Means (this function will be also used for mixture models initialization). The inputs to \texttt{init} are:
  \begin{enumerate}
  \item $X$: an $n \times d$ Numpy array of $n$ data points, each with $d$ features
  \item $K$: number of mixtures;
  \end{enumerate}
  To produce the input for \texttt{init} you will use \texttt{readToyData} for $K= [3, 6, 9]$. 

  \item Let $x$ be a data point. Recall the mixture model presented in class:
  $$P(x | \theta) = \sum^{K}_{j=1} p(j| \theta)p(x | j, \theta),$$ 
  where $\theta$ denotes the parameters of the model. Once you have learned a model for K-means, its easy to label each data point. However, labeling for mixture models would be different. Explain it in terms of how points get assigned to cluster types and what the difference between K-means and mixture models is. \note{TODO this question is the same as 1(f), which one should I keep?}
  
  [Hint: think about how memberships are assigned.]
  
 \item Consider a mixture model that uses a Gaussian as the conditional distribution given the hidden label. That is, $p(x | j, \theta) = N(x | \mu^{(j)}, \sigma^2_{j} I)$, where $\mu^{(j)}$ and $\sigma^2_jI$ are the unknown parameters for mixture component of type $j$.

    The goal of the EM algorithm is to estimate these unknown parameters by making use of observed data, say $x^{(1)},\ldots, x^{(N)}$. Starting with some initial guess about the unknown parameters, the E-Step keeps the model fixed (i.e., for each component $j$, the parameters $\mu^{(j)}$ and the $\sigma^2_{j}$ are held fixed) and computes the soft-assignments for each data point $x^{(t)}$ ($1 \le t \le N$). The M-step takes these soft-assignments as fixed and computes the maximum-likelihood estimates of the parameters $\mu^{(j)}$ and $\sigma_j^2I$ for each component $j$ ($1 \le j \le K$)---notice here an analogy to K-means, which, given assignment of data points to their clusters, computes the centroids of each cluster.

    \textbf{Task:} Implement the EM algorithm for a Gaussian mixture model (as recalled above). Write a Python function
    \texttt{mixGauss} in \texttt{project3\_student.py}. Your inputs should be
    \begin{enumerate}
    \item $X$: an $N \times D$ Numpy array of n data points, each with d features
    \item $K$: number of mixtures; 
    \item $M$: $K \times D$ Numpy array, each row corresponds to a mixture mean vector;
    \item $P$: $K \times 1$ Numpy array, each entry corresponds to the weight for a mixture;
    \item Var: $K \times 1$ Numpy array, each entry corresponds to the variance for a mixture;
    \end{enumerate}
    Your outputs should be
    \begin{enumerate}
    \item $M$: updated version of input $M$ (row $j$ of $M$ corresponds to mean $\mu^{(j)}$);
    \item $P$: updated version of input $P$ (entry $j$ corresponds to mixture weight $p_j$);
    \item Var: updated version of input Var (entry $j$ corresponds to the variance $\sigma_j^2$);
    \item post: $N \times K$ Numpy array; entry $i,j$ corresponds to the posterior probability $p(j|x^{(i)})$;
    \item LL: Numpy array which records the log-likelihood after each M-Step of EM.
    \end{enumerate}

\item \note{Suvrit comment: Probably delete this} The EM algorithm has a chicken an egg problem. Before it can evaluate a better gaussian model, it needs soft-clustering assignments and for re-evaluating soft-clustering assignments it needs some new but fixed gaussian model to evaluate the soft-clusterings. Hence, we need to break this chicken and egg problem by providing some initialization for the parameters so that EM can start working.

    \textbf{Task:} Think of some adversarial way to initialize the EM algorithm and \textbf{explain} why that initialization might not be infer the hidden labels in a good way.

\item Now that you have considered an initialization for EM, \textbf{run} your implementation of the EM algorithm using the \texttt{init} function we provided with initialization \note{TODO HE initialization}. Compute and report the log-likelihood of the parameters you learned and terminate the algorithm when the difference between iterations is less than $10^{-4}$ \note{TODO He precision} for clusters $k=5$ \note{TODO He number of clusters}.

\item One of the major problems with the EM algorithm is that it can get stuck in a local minimum (just like K-means has). Hence, to fix that one has to try different initializations and see which ones provides a better mixture models. One way is to choose among different initializations schemes. Sometimes, one effective way to fight advisers is to be random and unpredictable. 

\textbf{Task: } Try out different random initializations \note{TODO What should we ask them to deliver here?} with $K = 5$ and provide the best and worst log-likelihoods that you got. Provide 2 plots, one of the results of the best initializations and one of the worst. Make sure to try enough initializations, since you don't know which one will provide a solution close to the global optimum. Try at least 50 different random initializations. Comment on the difficulties and issues that your EM implementation.  Did the log-likelihood values you got make sense. \note{TODO He: for randomized init. how are we checking that their solutions are correct?}

[Hint: did it converge to solutions very different from the previous part? Did any of them localize around a single point?]

\item In your previous question, you might have found that your solutions converged to a single point and had a variance of zero. To fix this issue we need to change the EM algorithm slightly. In this question, since we know that the ratings are integers between 1 to 5 we can lower bound the variance by $\frac{1}{2}^2$ so that no variance goes to zero. Hence, change the update step of your variance in the M-step to be:

$$\sigma^2_i = \max \left\{ \frac{1}{2}^2 , \frac{1}{dn}\sum^n_i p(j | t) \| \mu^{(j)} - x^{(t)} \|^2 \right\}$$

\textbf{Task: }Now with this different M-step, try to re-run your randomized implementation again. Did you encounter the same difficulties as in the previous part? Does your solutions seem more close to the optimal solution?

%%I'd like to provide an appendix, mentioning the possible reasons for the initialization failure and how to fix them. For example, mentioning regularization I think is pretty important, even if we don't cover it. Also, commenting that this issue might be from a bad number of cluster being chosen.

\item Your next task is to understand how K-means clustering differs from the soft-clustering induced by learning a mixture model. Using the parameters estimated in part (c) and the function \texttt{plot2D}, \textbf{plot} the clusters each of the 2D Gaussians and \textbf{include} it in your write up. \textbf{Explain} why in the mixture model it does not make sense to deterministically assign any data point to a Gaussian and hence highlight its difference from K-means clustering. Do points that are very far away from cluster centroids still have a chance to be assigned to any cluster in EM? What about in K-means?

\item Now we will try to choose the number of mixture components ($K$) that EM should learn. \textbf{Explain} why choosing a value of $K$ that achieves the highest log-likelihood might not be the best criterion for selecting $K$.

\item One way to avoid the issues addressed in part (e) is to penalize a high number of parameters. \textbf{Explain} how the Bayesian Information Criterion (BIC) addresses the issue brought up in part (e) and why it might be a better function for choosing $K$.

\item Implement the Bayesian Information Criterion (BIC) for selecting the number of mixture components. \textbf{Run} your code and choose the best value of $K$ from the choices $\{ 5, 10, 15, 20 , 30\}$ \note{TODO He: values of K to choose from}.\\
    Write a python function \texttt{BICmix}. The inputs are:
    \begin{enumerate}
    \item $X$: an $N \times D$ Numpy array of $N$ data points, each with $D$ features
    \item $P$: $K \times 1$ Numpy array, each entry corresponds to the weight for a mixture component;
    \item Var: $K \times 1$ Numpy array, each entry corresponds to the variance for a mixture component;
    \end{enumerate}
    The output should be:
    \begin{enumerate}
    \item $K$: number of mixture components
    \item $M$: $K \times D$ Numpy array, each row corresponds to a mixture mean vector;
    \end{enumerate}
\end{enumerate}

\item {\bf Part 2} EM for predicting movie ratings via \emph{matrix completion}.

In this part of the project we will use the EM algorithm for matrix completion. Let $X$ denote the $N \times D$ data matrix. The rows of this matrix correspond to users and the columns correspond to movies. A single entry $x^{(u)}_{j}$ (or in matrix indexing notation $x_{uj}$) indicates the rating user $u$ gave to movie $j$, and this rating is a single number that lies in the set $\{1,2,3,4,5\}$.

In a realistic setting, most of the entries of $X$ will be missing, because a user may have watched / rated only a few movies. Thus, we use the set $C_{u}$ to denote the collection of movies (column indices) that user $u$ has rated. Also, let $H_{u}$ denote the set of movie indices that a user has not watched. Notice that $C_{u} \cup H_{u} = \{1,\ldots,D\}$. To denote a subset of the movies a particular user has watched we write $x_{C_u}^{(u)}$, which is a vector with $|C_u|$ entries. Similarly $x_{H_u}^{(u)}$ denotes the vector of hidden / unknown entries (the ratings we wish to estimate).

For example, if user $1$ has the ratings vector $x^{(1)} =  (5, 4, ?, ?, 2)$, then $C_{1} = \{1, 2, 5\}$ and $H_{1} = \{ 3, 4\}$ and $x^{(1)}_{C_1} = (5, 4, 2)$.

Our goal is to use a mixture model to generate the missing entries of the matrix $X$ (thus the name ``matrix completion''). We will estimate the parameters of the mixture model using EM.

\begin{enumerate}
\item The mixture model from Part 1 assigns the probability density $P(x^{(u)} | \theta) = \sum^{K}_{j=1} p_j N(x^{(u)}; \mu^{(j)}, \sigma^2_j I)$ to the vector $x^{(u)}$. However, since we have missing entries, i.e., not all entries of $x^{(u)}$ are known, we will just use only the observed data and compute $P(x^{(u)}_{C_u} | \theta)$. \textbf{Show} that the correct equation for $P(x^{(u)}_{C_u}  | \theta)$ (i.e. for the probability of the current data point) is:

 $$P(x^{(u)}_{C_u}  | \theta) = \sum^{K}_{j=1} p_j N(x^{(u)}_{C_u} ; \mu^{(j)}_{C_u} , \sigma^2_j I_{|C_u| \times |C_u|}).$$

Make sure to mention why the covariance matrix has an identity matrix $ |C_u| \times |C_u| $ and not $ d \times d $.

 [Hint: note that the covariance matrix is a multiple of the identity].


 \item Now that you have a mixture model for each user, provide a possible interpretation to what this mixture model could mean in this application. Specifically, mention what the clustering type could mean.

\item Using the mixture density from part (a) for a partial data point $x^{(u)}_{C_u}$, we are ready to write the incomplete log-likelihood and maximize it to derive the M-step of the EM algorithm.
To that end, we will maximize the following incomplete log-likelihood:
$$ l(\theta) = \sum_{u=1}^N \left[ \sum^{K}_{j=1} p(j|u) \log\bigl( p_j N(x^{(u)}_{C_u} | \mu^{(j)}_{C_u}, \sigma^2_j I_{|C_u| \times |C_u|} ) \bigr) \right]$$
where the posterior probability $p(j|u)$ can be interpreted as the soft-assignment of the data point $x^{(u)}_{C_u}$ to the mixture component $j$. To maximize $l( \theta)$, we keep the probabilities $p(j|u)$ (the soft-assignments) fixed, and we maximize over the model parameters. In this part we will \textbf{derive} the M-step that results after maximizing the above likelihood with respect to the model parameters.

\begin{enumerate}
\item  First {\bf derive } what the update equation should be for $\mu^{(j)}_{C_u}$ by considering each movie in the mean vector i.e. by considering the average rating $\mu^{(j)}_{l}$ for cluster type $j$ and movie $l$. In other words take the partial derivative with respect $\mu^{(j)}_{l}$ and setting the derivative to zero.

[Hint: notice that you are deriving the update for a single movie, therefore, make sure you only consider users that rated the particular movie being considered]
\item Now that we have updated what the new means of our gaussian should be we will update what the spread of them should be.
To do that take the partial derivative of the soft-counts log-likelihood function $l(\theta)$ with respect to $ \sigma^2_{j} $ for each different type of user $j$.
Notice that we are taking the derivative with respect to the variance not the standard deviation

[Hint: these facts might come in handy: The determinant of a diagonal matrix is the product of its diagonal entries and the inverse of diagonal matrix is the reciprocal of each entry]

\item
 \note{Note for Tommi and Suvrit, from Brando: I added this paragraph to make explicit every parameter that they will need for the M-setp. I also added some short explanation for it. What do you think?}
The last parameter to update will be the mixing proportions $p_j$. This quantity must satisfy the constraint $ \sum^{k}_{j=1} p_j = 1$, hence it needs the Lagrange multipliers to maximize correctly. It can be shown that the correct update in this case is:

$$ p_j = \frac{\sum^{n}_{1} p(j|u)}{n}$$

Can you provide an brief intuitive interpretation of this quantity?
[Hint: it might help to compare it to the mixing proportions $p_j$ when we have hard counts]\\

Notice that you will need \textbf{all} these three equations to provide a correct update for the M-step.
 
 \note{This is the question version of how to mathematically derive $p_j$. If we don't want the students to derive it we can just keep the explanation version previously described.} 
 \note{Suvrit note: Delete:} The last parameter to update will be the mixing proportions $p_j$. This quantity must satisfy the constraint $ \sum^{k}_{j=1} p_j = 1$, hence it needs to take a little more care to optimize it. Show that the correct update in this case is:

$$ p_j = \frac{\sum^{n}_{1} p(j|u)}{n}$$

by incorporating the constraint in the optimization problem through the Lagrange multipliers.

[Hint 1: write the expression of the Lagrangian using $ \lambda (1 -\sum^{k}_{j} p_j) $.]

[Hint 2: take the derivative with respect to $p_j$ and $\lambda$ and then manipulate the equation to not have $\lambda$ in any of the expressions.]

\end{enumerate}

\item In the E-Step of the EM algorithm, one uses the updated model parameters to re-estimate  the soft-assignments $p(j|u)$. \textbf{Write} down the formula for the E-Step that shows how to update $p(j|u)$ [Hint: the parameters $\mu^{(j)}_{C_u}$, $\sigma^2_j$, $p_j$ are to be held fixed in this step].

\item Next, \textbf{implement} the EM algorithm that will be run on the partially observed model. Use the E- and M- steps you derived in parts (b) and (c) above.\\
    Write a python function \texttt{mixGaussMiss}. Your inputs should be
    \begin{enumerate}
    \item $X$: an $N \times D$ Numpy array of $N$ data points, each with $D$ features;
    \item $K$: number of mixture components;
    \item $M$: $K \times D$ Numpy array, each row corresponds to a mixture mean vector;
    \item $P$: $K \times 1$ Numpy array, each entry corresponds to the weight for a mixture component;
    \item Var: $K \times 1$ Numpy array, each entry corresponds to the variance for a mixture component;
    \end{enumerate}
    Your outputs should be
    \begin{enumerate}
    \item $M$ updated version of input $M$, $K \times D$ Numpy array, each row corresponds to a mixture mean;
    \item $P$: updated version of input P, $K \times 1$ Numpy array, each entry corresponds to the weight for a mixture;
    \item Var: updated version of input Var, $K \times 1$ Numpy array, each entry corresponds to the variance for a mixture;
    \item post: $N \times K$ Numpy array, each row corresponds to the soft counts for all mixtures for an example
    \item LL: Numpy array, which records the loglikelihood value for each iteration of EM.
    \end{enumerate}
    
\item Finally, \textbf{run} your EM algorithm using the initialization (function \texttt{init}) to learn the model parameters on the incomplete data set (\texttt{1000mat.txt}) with initialization BLAH, number of clusters BLAH, precision BLAH  \note{TODO He: INITIALIZATION, PRECISION, NUMBER OF CLUSTERS,}. The correct log likelihood should be BLAH \note{TODO He put log-likelihood}. \note{\bf The correct log-likelihoods should be provided for the toy dataset, and not for the actual task. The initial checkpointing should happen for the case where they are still trying to implement EM (hence, move to question 1)}

\note{Tommi: EM for matrix completion is subtly different from the first EM. Hence, we need so e way for the students to check that their implementation is correct and also, the TAs need some way to check that their implementation is correct too. He and I had some ideas but weren't sure if you had any suggestions. There are numerical issues in compute the log-likelihood for example.}

\note{TODO He: initialization, precision, clusters, likelihood}

\item You are given both the complete matrix (\texttt{1000mat.txt}) and the incomplete matrix\\ (\texttt{1000mat\_complete.txt}).  Fill the missing entries from the learned the parameter. Now, we are ready to compare your completed matrix with the original complete matrix. We have provided a distance metric between matrices. \textbf{Run} the Frobenius norm distance function \texttt{DISTANCE FUNCTION NAME FORBENIUS} and compute the difference between the recovered matrix and the true matrix. \note{TODO: He, provide the correct file names, and forbenious norm equations}
% [Reference: You could find the definition of Frobenius norm at \texttt{http://mathworld.wolfram.com/FrobeniusNorm.html}]

\end{enumerate}


\end{enumerate}




%%%
%%%%%
%%%

\end{document}
