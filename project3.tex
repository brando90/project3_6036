\input 6036tex

\usepackage[pdftex]{graphicx}
\usepackage{amsmath, amsthm, amssymb, amsfonts, mathtools, graphicx, enumerate}
\usepackage{times}
\usepackage{booktabs}
\usepackage{url}
\usepackage{enumerate}
\usepackage{enumitem}

\setlength{\parindent}{0pt}
\setlength{\parskip}{1ex}

\newcommand{\answer}[1]{{\mbox{}\color{red}{#1}}}
\newcommand{\emptycheck}{\text{(\hspace{-.75ex}(\hspace{3ex})\hspace{-.75ex})}}
\newcommand{\checkans}[1]{\text{(\hspace{-.75ex}(\hspace{1ex}{#1}\hspace{1ex})\hspace{-.75ex})}}
\newcommand{\argmax}{{\mbox{arg}\hspace{-.1ex}}\max}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\psetheader{Spring 2014}{Project 3: Pun With Words
$ \;$ Issued: Tues., 4/15 Due: Fri., 4/25 at 9am}

{\bf Project Submission: Please submit two files---a \emph{single} PDF file containing all your answers, code, and graphs, and a
\emph{second} .zip file containing all the code you wrote for this
project, to
the Stellar web site by 9am, April 25th.}

%%
%%%%%%%%%
%%
\textbf{Introduction}

Your task is to build mixture model for collaborative filtering. In this project you will be given a fraction of the data matrix containing users and movie ratings from the netflix database. We have processed a subset of Netflix's data such that you get access to all the ratings for a subset of movies. The goal of this project will be to use the hidden structure in the different types of users that exist using the EM algorithm, then with the knowledge and this hidden structure we hope to be able to complete a partially observed rating matrix as an end goal.

\begin{enumerate}

\item {\bf Part 1} Toy Data Set

For this part of the project you will explore the connections of the results that clustering gives versus the clusters that EM algorithm gives.

\begin{enumerate}
  \item Use the toy data set and the k-means code we provided to plot different clusters for cluster sizes $k = [5, 10, 15]$. Notice that each data point is fully assigned to a single cluster. Meaning, that each point can only have one underlying hidden label for this model.
  \item Recall the mixture model that we was presented in class $P(x^{(t)} | \theta) = \sum^{K}_{j=1} p(j| \theta)p(x^{(t)} | j, \theta) $. A data point may be generated by first choosing a cluster and then choosing a data point x according to that cluster. Conceptually, explain how once you have cluster assignments given by k-means, how the data generation process for your model for k-means would differ from one from a mixture model. [Hint: k-means divides the plane with Voronoi diagrams but a mixture model allows weighted averages across cluster types].
  \item Consider a mixture model that uses a Gaussian as the conditional distribution given the hidden label i.e. $p(x^{(t)} | j, \theta) = N(x^{(t)}| \mu^{(j)}, \sigma^2_{j} I)$.  We will try to understand the EM algorithm on this model. 
 Recall that the EM algorithm aims to learn these parameters by sequential minimization (similar to how we presented clustering and boosting). 
In the E-step we will have the model fixed (i.e. hold the $ \mu^{(j)}$ and the $\sigma^2_{j}$ fixed, similar to holding the centroids fix) and compute the soft count assignments for each data point i.e. the posterior probability $p(j | x^{(t)}) = p(j|t)$. The M-step will receive these soft-counts and treat them as fixed. Now that we have these soft counts, we will compute the maximum-likelihood estimates for our model but with the hard-counts replaced by the soft count. 

Now that you've recalled the EM algorithm, please implement it and run it in the toy data set with a random initialization. 

\item In this section we will try to understand how clustering is different with the mixture model and the learned parameters from the EM algorithm. With the learned parameters from part (c), plot the clusters by providing 3 contour curves for each of the gaussians learned. Explain why choosing contour curves that are evenly spaced out is not a good visualization of these gaussians. Furthermore explain why having some of the area that these contour curves cover intersect, a good visualization for the mixture models. Moreover, explain why in the mixture model it does not make sense to deterministically assign any data point to a gaussian and hence highlight its difference from clustering. Do points that are very far away from  clusters still have a chance to be assigned to any cluster in EM? What about in clustering?

\item Now we will try to choose a good number of mixing components for the EM algorithm to learn the parameters. Explain why choosing a value of $k$ that achieves the highest log-likelihood might not be the best criterion for selecting the number of mixing components.

\item One way to avoid the issues addressed in part (e) is to penalize a high number of parameters. Explain how the Bayesian Information Criterion (BIC) addresses any of the issue brought up in part (e) and why it might be a better function for choosing the number of mixture components.

\item Implement the Bayesian Information Criterion (BIC) for selecting the number of mixture components. Choose the best value of k in the range $[ 5, 10, 15, 20 , 30]$.


\end{enumerate}

\item {\bf Part 2} Matrix Completion

For this part of the project we will use the EM algorithm for matrix completion. We will use X to denote the data matrix. It will be an $n \times d$ matrix, meaning that there will be $n$ rows and $d$ columns. The rows of the data matrix indicate users and the columns indicate movies. A single entry $x^{(i)}_{j}$ of the matrix will indicate the rating person $i$ gave to movie $j$ and the rating will be in the set from 1 to 5, i.e. $x^{(i)}_{j} \in \{ 1, ..., 5\}$. 

However, in a real life setting, most of the entries will be missing i.e. a user will not have watched most of the movies so he would have not rated any of those movies. To indicate that, we will use the set $C_{u}$ which is the collection of movie indices that user $u$ has rated. Also, denote $H_{u}$ as the set of movie indices that a user has not watch. Notice that $C_{u} \cup H_{u} = \{1, ... , d \}$. To denote a subset of the movies a particular user has watch we will use the notation $x^{(u)}_{C_u}$ which is a vector with only $|C_u|$ entries. Similarly $x^{(u)}_{H_u}$ will be the vector that contains the hidden entries.

For example, if we have the user vector $x^{(1)} = < 5, 4, ?, ?, 2>$, then its complete entries are $C_{1} = \{ 1, 2, 5\}$ and $H_{1} = \{ 3, 4\}$. Also, its complete vector would be $x^{(1)}_{C_1} = < 5, 4, 2>$.

In this case, we will have a different mixture model and using it, we will derive the EM algorithm for the missing entires matrix completion problem.

\begin{enumerate}

\item Recall the mixture model from part 1: $P(x^{(u)} | \theta) = \sum^{K}_{j=1} p_j N(x^{(u)}; \mu^{(j)}, \sigma^2_j I) $. However, in the missing entries case, we would actually be interested in finding $P(x^{(u)}_{C_u} | \theta)$ (i.e. the vector without the missing entries since those are not observed). Explain (or prove) why the correct expression for $P(x^{(u)}_{C_u} | \theta)$ is$P(x^{(u)}_{C_u}  | \theta) = \sum^{K}_{j=1} p_j N(x^{(u)}_{C_u} ; \mu^{(j)}_{C_u} , \sigma^2_j I) $ [hint: notice that the covariance matrix has the identity matrix].

\item Now that we have the correct expression for the probability of a  partial data point $x^{(u)}_{C_u}$, we are ready to write the log-likelihood and maximize to derive the M-step of the EM algorithm. Recall from the notes that the complete log-likelihood using the hard-assignment:

$$ \sum^n_{u=1} \left[ \sum^{K}_{j=1} \delta(j|u) \log( p_j N(x^{(u)}_{C_u} | \mu^{(j)}_{C_u}, \sigma^2_j I) \right]$$

Notice that we used the probability we derived in part (a). In this case the purpose of the indicator function is to select the probability of data points that actually generated them. In this case it would be like knowing the true assignments and hence we choose the exact (log) mixture component that generated it. However, the whole point is to learn such a hidden assignment without knowing it. Hence, to this goal we will be more conservative and instead replace the hard-assignment with soft assignments. i.e. instead we will try to maximize the following function:

$$ \sum^n_{u=1} \left[ \sum^{K}_{j=1} p(j|u) \log( p_j N(x^{(u)}_{C_u} | \mu^{(j)}_{C_u}, \sigma^2_j I) \right]$$

where the posterior probability $p(j|u)$, can be interpreted as the soft assignment of the data point $x^{(i)}_{C_u}$ getting the assignment to the (hidden) mixture $j$. Finally we are ready to perform the maximization step of the EM algorithm. In the maximization step $p(j|u)$ (the soft cluster assignment), are fixed and we update the model parameters with respect to this fixed posterior. Therefore, derive the M-step of this partially observed model by maximizing the above likelihood with respect to the variables $\mu^{(j)}_{C_u}$, $\sigma^2_j$ and $p_j$.

\item In the E-step, the EM algorithm uses the updated parameters to re-evaluate the soft cluster assignments. Hence, write down the E-step [hint: the parameters of the model $\mu^{(j)}_{C_u}$, $\sigma^2_j$, $p_j$ are fixed in this step].

\item Now we are ready to implement the EM algorithm! Please implement the EM algorithm for the partially observed data model using your E and M steps from part (b) and (c).

\item Please run your EM algorithm using the initialization that we will provide here to learn the model parameters on the incomplete netflix data set. Now use the parameters you learned to complete the matrix.

\item Report the squared distance between entries you complete and the true entries.

\end{enumerate} 


\end{enumerate}




%%%
%%%%%
%%%

\end{document}