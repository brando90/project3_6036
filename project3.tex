\input 6036tex

\usepackage[pdftex]{graphicx}
\usepackage{amsmath, amsthm, amssymb, amsfonts, mathtools, graphicx, enumerate}
\usepackage{times}
\usepackage{booktabs}
\usepackage{url}
\usepackage{enumerate}
\usepackage{enumitem}

\usepackage{xcolor}

\setlength{\parindent}{0pt}
\setlength{\parskip}{1ex}

\newcommand{\answer}[1]{{\mbox{}\color{red}{#1}}}
\newcommand{\emptycheck}{\text{(\hspace{-.75ex}(\hspace{3ex})\hspace{-.75ex})}}
\newcommand{\checkans}[1]{\text{(\hspace{-.75ex}(\hspace{1ex}{#1}\hspace{1ex})\hspace{-.75ex})}}
\newcommand{\argmax}{{\mbox{arg}\hspace{-.1ex}}\max}

\newcommand{\note}[1]{\textcolor{red}{#1}}
\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\psetheader{Spring 2015}{Project 3: Which movies do Gaussians like?
$ \;$ Issued: Fri.\ 4/10 Due: Fri.\ 4/24 at 9am}

{\bf Project Submission: Please submit two files---a \emph{single} PDF file containing all your answers, code, and graphs, and a
\emph{second} .zip file containing all the code you wrote for this
project, to
the Stellar web site by 9am, April 24th.}

%%
%%%%%%%%%
%%
\textbf{Introduction}

Your task is to build a mixture model for collaborative filtering. You are given a data matrix containing movie ratings made by users; we have sampled this matrix from the Netflix database. Not all movies have been rated by all users, and the goal of this project is to use mixture modeling to predict the missing ratings. You will explore this task by using the Expectation Maximization (EM) algorithm that uses the hidden structure across different users. With the help of this hidden structure you will be able to predict the missing ratings. In other words, we have a partially observed rating matrix and we will use the EM algorithm to fill in the missing entries.

\begin{enumerate}

\item {\bf Part 1} Warm up.

For this part of the project you will compare clustering obtained via K-means to the (soft) clustering induced by EM.

\begin{enumerate}
  \item Use the toy data set (\texttt{toy\_data.txt}) and the K-means code (function \texttt{kMeans} in\\ \texttt{project3\_student.py}) provided to plot different clusters for cluster sizes $K = [5, 10, 15]$. Notice that when using K-means, each data point is fully assigned to a single cluster, that is, each point can have only one cluster label.\\
  \textbf{Task:} Submit a plot of the clustering of the toy data set after running the K-means code we provided.\\
  Note that K-means should be initialized before use. We have provided a function \texttt{init} to initialize K-Means; this will be also used for mixture models. The inputs to \texttt{init} are:
  \begin{enumerate}
  \item $X$: an $n \times d$ Numpy array of $n$ data points, each with $d$ features
  \item $K$: number of mixtures;
  \end{enumerate}
  To produce the input for \texttt{init} you will use \texttt{readToyData} and specify $K$. 

  \item Let $x$ be a data point. Recall the mixture model presented in class: $$P(x | \theta) = \sum^{K}_{j=1} p(j| \theta)p(x | j, \theta),$$ where $\theta$ denotes the parameters of the model. A data point may be generated by first choosing a cluster, and then choosing a data point $x$ according to that cluster's distribution. Once you have learned the centroids and the clusters given by K-means, how would the data generation process based on K-means differ from the data generation process given by a mixture model. [Hint: K-means partitions the space into clusters, whereas a mixture model allows weighted memberships across different clusters].
  \item Consider a mixture model that uses a Gaussian as the conditional distribution given the hidden label. That is, $p(x | j, \theta) = N(x | \mu^{(j)}, \sigma^2_{j} I)$, where $\mu^{(j)}$ and $\sigma^2_jI$ are the unknown parameters for mixture component of type $j$.

    The goal of the EM algorithm is to estimate these unknown parameters by making use of observed data, say $x^{(1)},\ldots, x^{(N)}$. Starting with some initial guess about the unknown parameters, the E-Step keeps the model fixed (i.e., for each component $j$, the parameters $\mu^{(j)}$ and the $\sigma^2_{j}$ are held fixed) and computes the soft-assignments for each data point. Thus, for each data point $x^{(t)}$ ($1 \le t \le N$) the E-step computes the posterior probabilities $p(j | x^{(t)})$. The M-step takes these soft-assignments as fixed and computes the maximum-likelihood estimates of the parameters $\mu^{(j)}$ and $\sigma_j^2I$ for each component $j$ ($1 \le j \le K$)---notice here an analogy to K-means, which, given assignment of data points to their clusters, computes the centroids of each cluster.

    \textbf{Task:} Implement the EM algorithm for a Gaussian mixture model (as recalled above). Write a Python function
    \texttt{mixGauss} in \texttt{project3\_student.py}. Your inputs should be
    \begin{enumerate}
    \item $X$: an $N \times D$ Numpy array of n data points, each with d features
    \item $K$: number of mixtures; 
    \item $M$: $K \times D$ Numpy array, each row corresponds to a mixture mean vector;
    \item $P$: $K \times 1$ Numpy array, each entry corresponds to the weight for a mixture;
    \item Var: $K \times 1$ Numpy array, each entry corresponds to the variance for a mixture;
    \end{enumerate}
    Your outputs should be
    \begin{enumerate}
    \item $M$: updated version of input $M$ (row $j$ of $M$ corresponds to mean $\mu^{(j)}$);
    \item $P$: updated version of input $P$ (entry $j$ corresponds to mixture weight $p_j$);
    \item Var: updated version of input Var (entry $j$ corresponds to the variance $\sigma_j^2$);
    \item post: $N \times K$ Numpy array; entry $i,j$ corresponds to the posterior probability $p(j|x^{(i)})$;
    \item LL: Numpy array which records the log-likelihood after each M-Step of EM.
    \end{enumerate}

\item \note{Probably delete this} The EM algorithm has a chicken an egg problem. Before it can evaluate a better gaussian model, it needs soft-clustering assignments and for re-evaluating soft-clustering assignments it needs some new but fixed gaussian model to evaluate the soft-clusterings. Hence, we need to break this chicken and egg problem by providing some initialization for the parameters so that EM can start working.

	\textbf{Task:} Think of some adversarial way to initialize the EM algorithm and \textbf{explain} why that initialization might not be infer the hidden labels in a good way.

\item Now that you have considered an initialization for EM, run your implementation of the EM algorithm using the \texttt{init} function we provided. Compute and report the log-likelihood of the parameters you learned after $T=[5, 10, 20, 50, 100]$ steps.

\item Your next task is to understand how K-means clustering differs from the soft-clustering induced by learning a mixture model. Using the parameters estimated in part (c) and the function \texttt{plot2D}, \textbf{plot} the clusters each of the 2D Gaussians and include it in your write up. \textbf{Explain} why in the mixture model it does not make sense to deterministically assign any data point to a Gaussian and hence highlight its difference from K-means clustering. Do points that are very far away from cluster centroids still have a chance to be assigned to any cluster in EM? What about in K-means?

\item Now we will try to choose the number of mixture components ($K$) that EM should learn. \textbf{Explain} why choosing a value of $K$ that achieves the highest log-likelihood might not be the best criterion for selecting $K$.

\item One way to avoid the issues addressed in part (e) is to penalize a high number of parameters. \textbf{Explain} how the Bayesian Information Criterion (BIC) addresses the issue brought up in part (e) and why it might be a better function for choosing $K$.

\item Implement the Bayesian Information Criterion (BIC) for selecting the number of mixture components. Choose the best value of $K$ from the choices $\{ 5, 10, 15, 20 , 30\}$.\\
    Write a python function \texttt{BICmix}. The inputs are:
    \begin{enumerate}
    \item $X$: an $N \times D$ Numpy array of $N$ data points, each with $D$ features
    \item $P$: $K \times 1$ Numpy array, each entry corresponds to the weight for a mixture component;
    \item Var: $K \times 1$ Numpy array, each entry corresponds to the variance for a mixture component;
    \end{enumerate}
    The output should be:
    \begin{enumerate}
    \item $K$: number of mixture components
    \item $M$: $K \times D$ Numpy array, each row corresponds to a mixture mean vector;
    \end{enumerate}
\end{enumerate}

\item {\bf Part 2} EM for predicting movie ratings via \emph{matrix completion}.

In this part of the project we will use the EM algorithm for matrix completion. Let $X$ denote the $N \times D$ data matrix. The rows of this matrix correspond to users and the columns correspond to movies. A single entry $x^{(u)}_{j}$ (or in matrix indexing notation $x_{uj}$) indicates the rating user $u$ gave to movie $j$, and this rating is a single number that lies in the set $\{1,2,3,4,5\}$.

In a realistic setting, most of the entries of $X$ will be missing, because a user may have watched / rated only a few movies. Thus, we use the set $C_{u}$ to denote the collection of movies (column indices) that user $u$ has rated. Also, let $H_{u}$ denote the set of movie indices that a user has not watched. Notice that $C_{u} \cup H_{u} = \{1,\ldots,D\}$. To denote a subset of the movies a particular user has watched we write $x_{C_u}^{(u)}$, which is a vector with $|C_u|$ entries. Similarly $x_{H_u}^{(u)}$ denotes the vector of hidden / unknown entries (the ratings we wish to estimate).

For example, if user $1$ has the ratings vector $x^{(1)} =  (5, 4, ?, ?, 2)$, then $C_{1} = \{1, 2, 5\}$ and $H_{1} = \{ 3, 4\}$ and $x^{(1)}_{C_1} = (5, 4, 2)$.

Our goal is to use a mixture model to generate the missing entries of the matrix $X$ (thus the name ``matrix completion''). We will estimate the parameters of the mixture model using EM.

\begin{enumerate}
\item The mixture model from Part 1 assigns the probability density $P(x^{(u)} | \theta) = \sum^{K}_{j=1} p_j N(x^{(u)}; \mu^{(j)}, \sigma^2_j I)$ to the vector $x^{(u)}$. However, since we have missing entries, i.e., not all entries of $x^{(u)}$ are known, we will just use only the observed data and compute $P(x^{(u)}_{C_u} | \theta)$. \textbf{Argue} that the correct expression for $P(x^{(u)}_{C_u}  | \theta)$ is:

 $$P(x^{(u)}_{C_u}  | \theta) = \sum^{K}_{j=1} p_j N(x^{(u)}_{C_u} ; \mu^{(j)}_{C_u} , \sigma^2_j I_{|C_u| \times |C_u|}).$$

Make sure to mention why the covariance matrix has an identity matrix $ |C_u| \times |C_u| $ and not $ d \times d $.

 [Hint: note that the covariance matrix is a multiple of the identity].


 \item Now that you have a mixture model for each user, provide a possible interpretation to what this mixture model could mean in this application. Specifically, mention what the clustering type could mean.

\item Using the mixture density from part (a) for a partial data point $x^{(u)}_{C_u}$, we are ready to write the incomplete log-likelihood and maximize it to derive the M-step of the EM algorithm.
To that end, we will maximize the following incomplete log-likelihood:
$$ l(\theta) = \sum_{u=1}^N \left[ \sum^{K}_{j=1} p(j|u) \log\bigl( p_j N(x^{(u)}_{C_u} | \mu^{(j)}_{C_u}, \sigma^2_j I_{|C_u| \times |C_u|} ) \bigr) \right]$$
where the posterior probability $p(j|u)$ can be interpreted as the soft-assignment of the data point $x^{(u)}_{C_u}$ to the mixture component $j$. To maximize $l( \theta)$, we keep the probabilities $p(j|u)$ (the soft-assignments) fixed, and we maximize over the model parameters. In this part we will \textbf{derive} the M-step that results after maximizing the above likelihood with respect to the model parameters.

\begin{enumerate}
\item  First {\bf derive } what the update equation should be for $\mu^{(j)}_{C_u}$ by considering each movie in the mean vector. In other words take the partial derivative with respect $\mu^{(j)}_{l}$ and setting the derivative to zero.

[Hint: notice that you are deriving the update for a single movie, therefore, make sure you only consider users that rated the particular movie being considered]
\item Now that we have updated what the new means of our gaussian should be we will update what the spread of them should be.
To do that take the partial derivative of the soft-counts log-likelihood function $l(\theta)$ with respect to $ \sigma^2_{j} $ for each different type of user $j$.
Notice that we are taking the derivative with respect to the variance not the standard deviation

[Hint: these facts might come in handy: The determinant of a diagonal matrix is the product of its diagonal entries and the inverse of diagonal matrix is the reciprocal of each entry]

\item
 \note{Note for Tommi  and Suvrit from Brando: I added this paragraph to make explicit every parameter that they will need for the M-setp. I also added some short explanation for it. What do you think?}
The last parameter to update will be the mixing proportions $p_j$. This quantity must satisfy the constraint $ \sum^{k}_{j=1} p_j = 1$, hence it needs the Lagrange multipliers to derive. It can be shown that the correct update in this case is:

$$ p_j = \frac{\sum^{n}_{1} p(j|u)}{n}$$

intuitively, it counts the fraction of soft counts out of the total counts that are assigned to cluster $j$. We are giving you it because you will need it to implement the M-step of the EM algorithm. Notice that in this section you don't need to submit anything. 
 
 \note{This is the question version of how to get $p_j$. If we don't want the students to derive it we can just keep the explanation version previously described.} 
 \note{Suvrit note: Delete:} The last parameter to update will be the mixing proportions $p_j$. This quantity must satisfy the constraint $ \sum^{k}_{j=1} p_j = 1$, hence it needs to take a little more care to optimize it. Show that the correct update in this case is:

$$ p_j = \frac{\sum^{n}_{1} p(j|u)}{n}$$

by incorporating the constraint in the optimization problem through the Lagrange multipliers.

[Hint 1: write the expression of the Lagrangian using $ \lambda (1 -\sum^{k}_{j} p_j) $.]

[Hint 2: take the derivative with respect to $p_j$ and $\lambda$ and then manipulate the equation to not have $\lambda$ in any of the expressions.]

\end{enumerate}

\item In the E-Step of the EM algorithm, one uses the updated model parameters to re-estimate  the soft-assignments $p(j|u)$. \textbf{Write} down the formula for the E-Step that shows how to update $p(j|u)$ [Hint: the parameters $\mu^{(j)}_{C_u}$, $\sigma^2_j$, $p_j$ are to be held fixed in this step].

\item Next, \textbf{implement} the EM algorithm that will be run on the partially observed model. Use the E- and M- steps you derived in parts (b) and (c) above.\\
    Write a python function \texttt{mixGaussMiss}. Your inputs should be
    \begin{enumerate}
    \item $X$: an $N \times D$ Numpy array of $N$ data points, each with $D$ features;
    \item $K$: number of mixture components;
    \item $M$: $K \times D$ Numpy array, each row corresponds to a mixture mean vector;
    \item $P$: $K \times 1$ Numpy array, each entry corresponds to the weight for a mixture component;
    \item Var: $K \times 1$ Numpy array, each entry corresponds to the variance for a mixture component;
    \end{enumerate}
    Your outputs should be
    \begin{enumerate}
    \item $M$ updated version of input $M$, $K \times D$ Numpy array, each row corresponds to a mixture mean;
    \item $P$: updated version of input P, $K \times 1$ Numpy array, each entry corresponds to the weight for a mixture;
    \item Var: updated version of input Var, $K \times 1$ Numpy array, each entry corresponds to the variance for a mixture;
    \item post: $N \times K$ Numpy array, each row corresponds to the soft counts for all mixtures for an example
    \item LL: Numpy array, which records the loglikelihood value for each iteration of EM.
    \end{enumerate}
\item Finally, \textbf{run} your EM algorithm using the initialization (function \textbf{init}) to learn the model parameters on the incomplete data set (\texttt{1000mat.txt}). And once you have learned the parameters using EM, you have at your hands a \emph{generative model}, which you should apply to complete the matrix (generate the missing entries from $H_u$ for all users).

\item You are given both the complete matrix (\texttt{1000mat.txt}) and the incomplete matrix\\ (\texttt{1000mat\_complete.txt}). Now that you have filled the missing entries, we are ready to compare it with original complete matrix. Report the Frobenius norm of the difference between your recovered matrix and the true matrix. \note{Just supply them the function: \texttt{dist} that takes as arguments two matrices of equal size and computes this score.}
% [Reference: You could find the definition of Frobenius norm at \texttt{http://mathworld.wolfram.com/FrobeniusNorm.html}]

\end{enumerate}


\end{enumerate}




%%%
%%%%%
%%%

\end{document}
