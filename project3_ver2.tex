\input 6036tex

\usepackage[pdftex]{graphicx}
\usepackage{amsmath, amsthm, amssymb, amsfonts, mathtools, graphicx, enumerate}
\usepackage{times}
\usepackage{booktabs}
\usepackage{url}
\usepackage{enumerate}
\usepackage{enumitem}

\usepackage{xcolor}

\setlength{\parindent}{0pt}
\setlength{\parskip}{1ex}

\newcommand{\answer}[1]{{\mbox{}\color{red}{#1}}}
\newcommand{\emptycheck}{\text{(\hspace{-.75ex}(\hspace{3ex})\hspace{-.75ex})}}
\newcommand{\checkans}[1]{\text{(\hspace{-.75ex}(\hspace{1ex}{#1}\hspace{1ex})\hspace{-.75ex})}}
\newcommand{\argmax}{{\mbox{arg}\hspace{-.1ex}}\max}

\newcommand{\note}[1]{\textcolor{red}{#1}}
\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\psetheader{Spring \note{2015}}{Project 3: \note{Which movies do I like?}
$ \;$ \note{Issued: Thursday., 4/10} Due: Fri., 4/24 at 9am}

{\bf Project Submission: Please submit two files---a \emph{single} PDF file containing all your answers, code, and graphs, and a
\emph{second} .zip file containing all the code you wrote for this
project, to
the Stellar web site by 9am, April 25th.}

%%
%%%%%%%%%
%%
\textbf{Introduction}

Your task is to build a mixture model for collaborative filtering. You are given a data matrix containing movie ratings made by users; we have sampled this matrix from the Netflix database. Not all movies have been rated by all users, and the goal of this project is to use mixture modeling to predict the missing ratings. You will explore this task by using the Expectation Maximization (EM) algorithm that uses the hidden structure across different users, and with the help of this hidden structure, you will be able to predict the missing ratings. In other words, we have a partially observed rating matrix, and we will use the EM algorithm to complete (fill in) the missing entries.

\begin{enumerate}

\item {\bf Part 1} Warm up example.

For this part of the project you will compare clustering obtained via K-means to the (soft) clustering induced by EM.

\begin{enumerate}
  \item Use the toy data set (\texttt{blah.csv}) and the K-means code (\texttt{kmeans.py}) provided to plot different clusters for cluster sizes $K = [5, 10, 15]$. Notice that when using K-means, each data point is fully assigned to a single cluster, that is, each point can have only one cluster label.
  \item Let $x$ be a data point. Recall the mixture model presented in class: $$P(x | \theta) = \sum^{K}_{j=1} p(j| \theta)p(x | j, \theta),$$ where $\theta$ denotes the parameters of the model. A data point may be generated by first choosing a cluster, and then choosing a data point $x$ according to that cluster's distribution (\note{What does this mean?}). Explain how once you have cluster assignments given by K-means would the data generation process based on a K-means model would differ from data generation via a mixture model. [Hint: K-means partitions the space into clusters, whereas a mixture model allows weighted memberships across different clusters].
  \item Consider a mixture model that uses a Gaussian as the conditional distribution given the hidden label. That is, $p(x | j, \theta) = N(x | \mu^{(j)}, \sigma^2_{j} I)$, where $\mu^{(j)}$ and $\sigma^2_jI$ are the unknown parameters for mixture component $j$ (label $j$). 

    The goal of the EM algorithm is to estimate these unknown parameters by making use of observed data, say $x^{(1)},\ldots, x^{(N)}$. Starting with some initial guess at the unknown parameters, the E-Step keeps the model fixed (i.e., for each component $j$, its parameters $\mu^{(j)}$ and the $\sigma^2_{j}$ are held fixed; this idea is similar to K-means holding the centroids fixed in one of its steps), and computes the soft-assignments for each data point. Thus, for each data point $x^{(t)}$ ($1 \le t \le N$) the E-step computes the posterior probabilities $p(j | x^{(t)})$. The M-step takes these soft-assignments as fixed, and computes the maximum-likelihood estimates of the parameters $\mu^{(j)}$ and $\sigma_j^2I$ for each component $j$ ($1 \le j \le K$) (notice the analogy to K-means, which, given assignment of data points to their clusters, computes the centroids of each cluster).

    \textbf{Task:} Implement the EM algorithm using a Gaussian mixture model (as recalled above), and run it in the toy data set with a \note{random initialization: WE have to provide the initialization so that the students can checkpoint their results: e.g., after running 100 iterations of EM, using our initialization, on this data, the log-likelihood they obtain should match the one we provide.}

    \note{The question below uses the word clustering, when really the students are only running K-means induced clustering. Perhaps good to clarify that hard clustering is meant.}
\item Your next task is to understand how \note{K-means} clustering differs from the soft-clustering induced by learning a mixture model. Using the parameters estimated in part (c) (\note{obtained after 100 iterations of EM}), \textbf{plot} the clusters (\note{\textbf{visualize} the soft-clusters?}) by drawing 3 contours for each of the \text{2D?} Gaussians. \textbf{Explain} why choosing contour curves that are evenly spaced is not a good visualization of these Gaussians. Further, \textbf{explain} why is having some of the area covered by these contour curves cover intersect a good visualization for mixture models \note{This is a vague task...}. Also \textbf{explain} why in the mixture model it does not make sense to deterministically assign any data point to a Gaussian and hence highlight its difference from K-means clustering. Do points that are very far away from cluster \note{centroids} still have a chance to be assigned to any cluster in EM? What about in K-means?

\item Now we will try to choose the number of mixture components ($K$) that EM should learn. \textbf{Explain} why choosing a value of $K$ that achieves the highest log-likelihood might not be the best criterion for selecting $K$.

\item One way to avoid the issues addressed in part (e) is to penalize a high number of parameters. \textbf{Explain} how the Bayesian Information Criterion (BIC) addresses the issue brought up in part (e) and why it might be a better function for choosing $K$.

\item Implement the Bayesian Information Criterion (BIC) for selecting the number of mixture components. Choose the best value of $K$ from the choices $\{ 5, 10, 15, 20 , 30\}$.

\note{I have highlighted the deliverables for the questions using \textbf{bold}; please ensure that this is consistent, or at least for each question we clearly indicate what they have to deliver.}
\end{enumerate}

\item {\bf Part 2} EM for predicting movie ratings via \emph{matrix completion}.

In this part of the project we will use the EM algorithm for matrix completion. Let $X$ denote the $N \times D$ data matrix. The rows of this matrix correspond to users and the columns correspond to movies. A single entry $x^{(i)}_{j}$ (\note{Better to write: $x_{ij}$}) indicates the rating person user $i$ gave to movie $j$, and this rating is a single number that lies in the set $\{1,2,3,4,5\}$. 

In a realistic setting, most of the entries of $X$ are missing, because a user will not have watched most of the movies so he/she would have not rated the unwatched movies. Thus, we use the set $C_{u}$ to denote the collection of movies (column indices) that user $u$ has rated. Also, let $H_{u}$ denote the set of movie indices that a user has not watched. Notice that $C_{u} \cup H_{u} = \{1,\ldots,D\}$. To denote a subset of the movies a particular user has watched we write $x_{C_u}^{(u)}$, which is a vector with $|C_u|$ entries. Similarly $x_{H_u}^{(u)}$ denotes the vector of hidden / unknown entries (the ratings we wish to estimate).

For example, if user $1$ has the ratings vector $x^{(1)} =  (5, 4, ?, ?, 2)$, then $C_{1} = \{1, 2, 5\}$ and $H_{1} = \{ 3, 4\}$ and $x^{(1)}_{C_1} = (5, 4, 2)$.

Our goal is to use a mixture model to generate the missing entries of the matrix $X$ (thus the name ``matrix completion''). We will estimate the parameters of the mixture model using EM.

\begin{enumerate}
\item The mixture model from Part 1 assigns the probability density $P(x^{(u)} | \theta) = \sum^{K}_{j=1} p_j N(x^{(u)}; \mu^{(j)}, \sigma^2_j I)$ to the vector $x^{(u)}$. However, since we have missing entries, i.e., not all entries of $x^{(u)}$ are known, we will just use only the observed data and compute $P(x^{(u)}_{C_u} | \theta)$. \textbf{Argue} that the correct expression for $P(x^{(u)}_{C_u}  | \theta)$ is:

 $$P(x^{(u)}_{C_u}  | \theta) = \sum^{K}_{j=1} p_j N(x^{(u)}_{C_u} ; \mu^{(j)}_{C_u} , \sigma^2_j I_{|C_u| \times |C_u|}).$$ 
 
Make sure to mention why the covariance matrix has an identity matrix $ |C_u| \times |C_u| $ and not $ d \times d $.
 
 [Hint: note that the covariance matrix is a multiple of the identity].

 
 \item Now that you have a mixture model for each user, provide a possible interpretation to what this mixture model could mean in this application. Specifically, mention what the clustering type could mean.

\item Using the mixture density from part (a) for a partial data point $x^{(u)}_{C_u}$, we are ready to write the log-likelihood and maximize to derive the M-step of the EM algorithm. 

\note{The following text in blue color is not needed and should be deleted}
\color{blue}
Recall from the notes that the complete log-likelihood using the hard-assignment:

$$ l(\theta) = \sum^n_{u=1} \left[ \sum^{K}_{j=1} \delta(j|u) \log( p_j N(x^{(u)}_{C_u} | \mu^{(j)}_{C_u}, \sigma^2_j I_{|C_u| \times |C_u|}) \right]$$

Notice that we used the probability we derived in part (a). In this case the purpose of the indicator function is to select the probability of data points that actually generated them. In this case it would be like knowing the true assignments and hence we choose the exact (log) mixture component that generated it. However, the whole point is to learn such a hidden assignment without knowing it. Hence, to this goal we will be more conservative and instead replace the hard-assignment with soft assignments.
\color{black}

To that end, we will maximize the following log-likelihood:
$$ l(\theta) = \sum_{u=1}^N \left[ \sum^{K}_{j=1} p(j|u) \log\bigl( p_j N(x^{(u)}_{C_u} | \mu^{(j)}_{C_u}, \sigma^2_j I_{|C_u| \times |C_u|} ) \bigr) \right]$$
where the posterior probability $p(j|u)$ can be interpreted as the soft-assignment of the data point $x^{(u)}_{C_u}$ to the mixture component $j$. To maximize the above log-likelihood, we keep the probabilities $p(j|u)$ (the soft-assignments) fixed, and we maximize over the model parameters. In this part we will \textbf{derive} the M-step that results after maximizing the above likelihood with respect to the model parameters.

\begin{enumerate}
\item  First derive what the update equation should be for $\mu^{(j)}_{C_u}$ by considering each movie in its vector. In other words take the partial derivative with respect $\mu^{(j)}_{l}$ and setting the derivative to zero. [Hint: notice that you are deriving the update for a single movie, hence, make sure you only consider users that rated that movie]
\item Now that we have updated what the new means of our gaussian should be we will update what the spread of them should be. 
To do that take the partial derivative of the soft-counts log-likelihood function $l(\theta)$ with respect to $ \sigma^2_{j=1} $ for each different type of user $j$. 
Notice that we are taking the derivative with respect to the variance not the standard deviation [Hint: these facts might be useful. The determinant of a diagonal matrix is the product of its diagonal entries and the inverse of diagonal matrix is the reciprocal of each entry]
\item The last parameter to update will be the mixing proportions $p_j$. This quantity must satisfy the constraint $ \sum^{k}_{j=1} p_j = 1$, hence it needs to take a little more care to optimize it. Show that the correct update in this case is:

$$ p_j = \frac{\sum^{n}_{1} p(j|u)}{n}$$

by incorporating the constraint in the minimization problem through the Lagrange multipliers. [Hint1: write the expression of the Lagrangian using $ \lambda (1 -\sum^{k}_{j} p_j) $.]
[Hint2: take the derivative with respect to $p_j$ and $\lambda$ and then manipulate the equation to not have $\lambda$ in any of the expressions.]

\end{enumerate}

\item In the E-Step of the EM algorithm, one uses the updated model parameters to re-estimate  the soft-assignments $p(j|u)$. \textbf{Write} down the formula for the E-Step that shows how to update $p(j|u)$ [Hint: the parameters $\mu^{(j)}_{C_u}$, $\sigma^2_j$, $p_j$ are to be held fixed in this step].

\item Next, \textbf{implement} the EM algorithm for running on the partially observed model. Use the E- and M- steps you derived in parts (b) and (c) above.

\item Finally, \textbf{run} your EM algorithm using the initialization (\texttt{file.csv?, read\_init.py}) to learn the model parameters on the incomplete data set (\texttt{netflix.csv}). And once you have learned the parameters using EM, you have at your hands a \emph{generative model}, which you should apply to complete the matrix (generate the missing entries).

\item You are given both the complete matrix and the uncompleted matrix. Now that you have filled the missing entries, we are ready to compare it with original complete matrix. Report the squared distance between entries you completed and the true entries. \note{This requires knowing the true entries? Fix this question / make more precise.}

\end{enumerate} 


\end{enumerate}




%%%
%%%%%
%%%

\end{document}